import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# ğŸ§  å®šç¾©æ¨¡å‹æ¶æ§‹ï¼ˆèˆ‡ä½ è¨“ç·´ç”¨çš„å®Œå…¨ä¸€æ¨£ï¼‰
class SmallQuantCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.quant   = torch.quantization.QuantStub()
        self.dequant = torch.quantization.DeQuantStub()

        self.conv1 = nn.Conv2d(1, 6, kernel_size=3, stride=1, padding=1)
        self.relu1 = nn.ReLU(inplace=True)
        self.pool1 = nn.MaxPool2d(2)

        self.conv2 = nn.Conv2d(6, 12, kernel_size=3, stride=1, padding=1)
        self.relu2 = nn.ReLU(inplace=True)
        self.pool2 = nn.MaxPool2d(2)

        self.gap = nn.MaxPool2d(4)
        self.fc  = nn.Linear(12, 10)

        # æ¨¡çµ„ fuse å° QAT æ¨¡å‹åŒ¯å…¥å¾ˆé‡è¦ï¼ˆéœ€è¦èˆ‡è¨“ç·´ä¸€è‡´ï¼‰
        torch.quantization.fuse_modules(self, ['conv1', 'relu1'], inplace=True)
        torch.quantization.fuse_modules(self, ['conv2', 'relu2'], inplace=True)

    def forward(self, x):
        x = self.quant(x)
        x = self.pool1(self.relu1(self.conv1(x)))
        x = self.pool2(self.relu2(self.conv2(x)))
        x = self.gap(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        x = self.dequant(x)
        return x

# âœ… è³‡æ–™å‰è™•ç†
def binarize_input(x):
    return (x > 0.5).float()

transform = transforms.Compose([
    transforms.CenterCrop(16),
    transforms.ToTensor(),
    transforms.Lambda(binarize_input)
])

# âœ… è¼‰å…¥æ¸¬è©¦è³‡æ–™é›†
test_ds = datasets.MNIST('./data', train=False, download=True, transform=transform)
test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)

# âœ… å»ºç«‹æ¨¡å‹ä¸¦é‡åŒ–
model = SmallQuantCNN()
model.eval()
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)  # æ³¨æ„ï¼šæ­¤è™•ä½¿ç”¨ PTQ prepare
quantized_model = torch.quantization.convert(model, inplace=False)

# âœ… è¼‰å…¥ä½ å·²ç¶“è½‰æ›å¥½çš„é‡åŒ–æ¬Šé‡
quantized_model.load_state_dict(torch.load("mnist_int8_2.pth", map_location='cpu'))
quantized_model.eval()

# âœ… è¨ˆç®—æº–ç¢ºåº¦
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        outputs = quantized_model(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

acc = 100 * correct / total
print(f"é‡åŒ–æ¨¡å‹æ¨è«–æº–ç¢ºç‡ï¼š{acc:.2f}%")


import torch

# å„²å­˜ä¸­é–“è¼¸å‡º
activations = {}

def get_activation(name):
    def hook(model, input, output):
        activations[name] = output.detach()
    return hook

# å»ºç«‹æ¨¡å‹ä¸¦è¼‰å…¥é‡åŒ–æ¬Šé‡
model = SmallQuantCNN()
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)
quantized_model = torch.quantization.convert(model, inplace=False)
quantized_model.load_state_dict(torch.load("mnist_int8_2.pth", map_location="cpu"))
quantized_model.eval()

# åŠ å…¥ hook
quantized_model.conv1.register_forward_hook(get_activation('conv1'))
quantized_model.conv2.register_forward_hook(get_activation('conv2'))
quantized_model.gap.register_forward_hook(get_activation('gap'))
quantized_model.fc.register_forward_hook(get_activation('fc'))


# æº–å‚™è³‡æ–™
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

def binarize_input(x):
    return (x > 0.5).float()

transform = transforms.Compose([
    transforms.CenterCrop(16),
    transforms.ToTensor(),
    transforms.Lambda(binarize_input)
])

test_loader = DataLoader(datasets.MNIST('./data', train=False, transform=transform), batch_size=1, shuffle=False)
i=0
# æ¨è«–å–®ä¸€æ¨£æœ¬ä¸¦é¡¯ç¤ºä¸­é–“è¼¸å‡º
with torch.no_grad():
    for images, labels in test_loader:
        outputs = quantized_model(images)
        pred = outputs.argmax(dim=1)
        print(f"é æ¸¬: {pred.item()}ï¼ŒçœŸå¯¦: {labels.item()}")
        print(quantized_model.conv1.weight())
        input()

        print("Conv1 Output INT8:\n", activations['conv1'].int_repr().squeeze().numpy())
        input('conv1 output:')
        print('images:', images)
        print("Conv1 Weights INT8:\n", quantized_model.conv1.weight().int_repr().squeeze().numpy())
        input('conv2 weight:')
        print("Conv2 Weight Scales:\n", quantized_model.conv2.weight().q_per_channel_scales().numpy())
        print("Conv2 Weight ZPs:\n", quantized_model.conv2.weight().q_per_channel_zero_points().numpy())
        print("Conv2 Bias:\n", quantized_model.conv2.bias().detach().numpy())
        print("Conv2 Output INT8:\n", activations['conv2'].int_repr().squeeze().numpy())
        # print("Conv2 Output Scale:\n", activations['conv2'].q_scale())
        # print("Conv2 Output ZP:\n", activations['conv2'].q_zero_point())
        input('conv2 output:')



        print(images)
        
        print("conv1 INT8 å€¼:", activations['conv1'].int_repr().squeeze().numpy())
        print("scale:", activations['conv1'].q_scale())
        print("zero_point:", activations['conv1'].q_zero_point())
        print("conv1 kernel INT8 å€¼:", quantized_model.conv1.weight().int_repr().squeeze().numpy())
        print("conv1 kernel scale:", quantized_model.conv1.weight().q_per_channel_scales().numpy())
        print("conv1 kernel zero_point:", quantized_model.conv1.weight().q_per_channel_zero_points().numpy())
        # print(state_dict["conv1.scale"])

        # print("conv1 bias INT32 å€¼:", quantized_model.conv1.bias().detach().numpy())

        print('conv1 æ¬Šé‡:', quantized_model.conv1.weight().int_repr().squeeze().numpy())
        print('conv2 è¼¸å…¥', activations['conv2'])
        print("conv2 INT8 å€¼:", activations['conv2'].int_repr().squeeze().numpy())
        print("scale:", activations['conv2'].q_scale())
        print("zero_point:", activations['conv2'].q_zero_point())
        print("conv2 kernel INT8 å€¼:", quantized_model.conv2.weight().int_repr().squeeze().numpy())
        # print("conv2 kernel scale:", quantized_model.conv2.weight().q_per_channel_scales().numpy())
        input('ckeck pool2d')
        # print("conv2 æ¯å€‹ output channel å°æ‡‰ç¬¬ 0 å€‹ input channel çš„ kernelï¼š")
        # print(quantized_model.conv2.weight().int_repr()[:, 0, :, :].numpy())  
        print("pool1 è¼¸å‡º:", activations['conv2'].int_repr().squeeze().numpy())
        print('activations = ', activations)
        if i ==1 :
            input('123')
        print("gap INT8 å€¼:", activations['gap'].int_repr().squeeze().numpy())
        print("scale:", activations['gap'].q_scale())
        print("zero_point:", activations['gap'].q_zero_point())

        print("fc INT8 å€¼:", activations['fc'].int_repr().squeeze().numpy())
        input('fc output:')
        print("scale:", activations['fc'].q_scale())
        print("zero_point:", activations['fc'].q_zero_point())

        i= i + 1

        # break  # åªçœ‹ä¸€å¼µåœ–

input()



# import os
# import numpy as np
# import torch
# def export_quantized_weights_to_txt(model, dir_path="quant_weights_txt"):
#     print("ğŸ” å°å‡ºé‡åŒ–æ¨¡å‹åƒæ•¸ç‚º .txt ...")
#     os.makedirs(dir_path, exist_ok=True)
#     state_dict = model.state_dict()

#     for k, v in state_dict.items():
#         print(f"ğŸ” è™•ç† {k} ...")

#         # âœ… ç‰¹åˆ¥è™•ç† packed fc å±¤ï¼ˆä¸€å®šè¦å…ˆåˆ¤æ–·ï¼‰
#         if "fc._packed_params._packed_params" in k:
#             print("âš ï¸ ç‰¹åˆ¥è™•ç† packed fc å±¤ ...")
#             weight, bias = v  # v æ˜¯ tuple: (weight_tensor, bias_tensor)

#             arr_int8 = weight.int_repr().cpu().numpy()
#             np.savetxt(os.path.join(dir_path, "fc_int8.txt"), arr_int8.flatten(), fmt="%d")
#             np.savetxt(os.path.join(dir_path, "fc_bias.txt"), bias.detach().cpu().numpy().flatten(), fmt="%.6f")

#             qscheme = weight.qscheme()
#             if qscheme == torch.per_tensor_affine:
#                 scale = weight.q_scale()
#                 zero_point = weight.q_zero_point()
#                 np.savetxt(os.path.join(dir_path, "fc_scale_bias.txt"), [scale], fmt="%.8f")
#                 np.savetxt(os.path.join(dir_path, "fc_zero_point_bias.txt"), [zero_point], fmt="%d")
#             elif qscheme == torch.per_channel_affine:
#                 scales = weight.q_per_channel_scales().cpu().numpy()
#                 zero_points = weight.q_per_channel_zero_points().cpu().numpy()
#                 axis = weight.q_per_channel_axis()
#                 np.savetxt(os.path.join(dir_path, "fc_scale_bias.txt"), scales, fmt="%.8f")
#                 np.savetxt(os.path.join(dir_path, "fc_zero_point_bias.txt"), zero_points, fmt="%d")
#                 with open(os.path.join(dir_path, "fc_axis.txt"), "w") as f:
#                     f.write(f"{axis}\n")
#             else:
#                 print(f"âš ï¸ ä¸æ”¯æ´çš„ fc qscheme: {qscheme}")
#             continue  # âœ… å·²è™•ç†å®Œ fcï¼Œè·³éä»¥ä¸‹æµç¨‹

#         # âœ… è·³éé Tensor çš„ä¸€èˆ¬æƒ…æ³
#         if not isinstance(v, torch.Tensor):
#             continue

#         # âœ… è™•ç† conv1ã€conv2 ç­‰å¸¸è¦ Tensor æ¬Šé‡æˆ– bias
#         base_name = k.replace(".weight", "").replace(".bias", "")

#         if v.is_quantized:
#             arr_int8 = v.int_repr().cpu().numpy()
#             qscheme = v.qscheme()

#             if qscheme == torch.per_tensor_affine:
#                 scale = v.q_scale()
#                 zero_point = v.q_zero_point()
#                 np.savetxt(os.path.join(dir_path, f"{base_name}_int8.txt"), arr_int8.flatten(), fmt="%d")
#                 np.savetxt(os.path.join(dir_path, f"{base_name}_scale_bias.txt"), [scale], fmt="%.8f")
#                 np.savetxt(os.path.join(dir_path, f"{base_name}_zero_point_bias.txt"), [zero_point], fmt="%d")

#             elif qscheme == torch.per_channel_affine:
#                 scales = v.q_per_channel_scales().cpu().numpy()
#                 zero_points = v.q_per_channel_zero_points().cpu().numpy()
#                 axis = v.q_per_channel_axis()
#                 np.savetxt(os.path.join(dir_path, f"{base_name}_int8.txt"), arr_int8.flatten(), fmt="%d")
#                 np.savetxt(os.path.join(dir_path, f"{base_name}_scale_bias.txt"), scales, fmt="%.8f")
#                 np.savetxt(os.path.join(dir_path, f"{base_name}_zero_point_bias.txt"), zero_points, fmt="%d")
#                 with open(os.path.join(dir_path, f"{base_name}_axis.txt"), "w") as f:
#                     f.write(f"{axis}\n")

#             else:
#                 print(f"âš ï¸ ä¸æ”¯æ´çš„é‡åŒ–æ–¹å¼ï¼š{qscheme}ï¼Œè·³é {k}")

#         else:
#             # éé‡åŒ– Tensorï¼Œä¾‹å¦‚ bias
#             arr = v.detach().cpu().numpy()
#             np.savetxt(os.path.join(dir_path, f"{base_name}_bias.txt"), arr.flatten(), fmt="%.6f")

#     print(f"âœ… åŒ¯å‡ºå®Œæˆï¼šæ‰€æœ‰é‡åŒ–åƒæ•¸å·²å„²å­˜åˆ° {dir_path}/")

# # âœ… æª¢æŸ¥ conv1 æ¬Šé‡çš„é‡åŒ–æ–¹å¼
# w = quantized_model.conv1.weight()
# print("conv1 qscheme:", w.qscheme())  # æœƒå°å‡º torch.per_channel_affine æˆ– torch.per_tensor_affine

# export_quantized_weights_to_txt(quantized_model, "quant_weights_txt")
